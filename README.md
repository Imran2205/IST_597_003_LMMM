# Assessment of Multimodal Models' Capabilities in Understanding Image and Video Data

#### Method:
In this work, we evaluate the capabilities of multimodal models in understanding visual data. Specifically, we selected the LLaVA multimodal model due to its open-source availability and ease of use. Additionally, its performance is comparable to that of GPT-4. To utilize LLaVA as a chat agent, we initiated the LLaVA server controller on localhost and ran another worker with a preferred LLaVA variant on localhost. Following this, we configured Autogen with LLaVA, enabling its use as a chat agent. We then prompted the agent with images and textual descriptions of tasks to be executed. 

The images were sourced from a synthetic dataset named "Playing for Benchmarks," which contains images from gameplay videos. We tasked the model with describing the given image, identifying different objects, and extracting spatial information (e.g., bounding boxes). However, the model was unable to detect all objects present in the image.

In an alternative approach, we attempted to use LLaVA for generating descriptions of small video clips. We trimmed clips from the aforementioned dataset and extracted keyframes using a robust keyframe extraction tool, Katna. These frames were then passed to a detector model. Initially, we utilized YOLOv7 due to its real-time performance and ease of use. Despite being open-source, the off-the-shelf YOLOv7 detector is trained on the MS COCO dataset, which only includes 80 object categories. In the future, we aim to leverage the â€œSegment Anything" model to enhance spatial information extraction from the images. YOLOv7 was able to detect multiple object categories and provide bounding box coordinates for each detected instance. Subsequently, we created a knowledge base from the predictions of all frames, which was then passed to our chat agent. The agent was tasked with generating a video description based solely on this knowledge base, without any images. Although the model provided a high-level description of the video, it failed to capture the temporal relationships between frames.

We employ state of the art multi-object tracker (QDTrack, ByteTrack) to track objects in the video and get the trajectory of each object.
We run the tracker on video and extract the trajectories of all the objects across the frames of the video. Based on the extracted trajectories, we construct a knowledge base. This knowledge base is provided to the LLM agent. The code for the tracker experiment can be found in ''tracker_output_visualization.ipynb'' file. Extracted trajectories are available in the data folder.

For future iterations, we plan to employ a tracker to compute the trajectories of different objects and incorporate this trajectory information into the knowledge base. This will enable the model to understand the temporal relationships between frames. Additionally, we will include various attributes of the objects, such as color and shape, to enrich the model's descriptive capabilities.

#### Experiment and Results:
When the LMMM is tasked with describing an image, it can generate a high-level description that includes information about some of the objects present. However, its ability to detect all available objects is limited, often identifying only a few. To obtain detailed information about each object, it is necessary to inquire explicitly. To address this limitation, we integrate a detector model to extract spatial information and construct a knowledge base. This enables the LMMM to utilize the knowledge base for more comprehensive descriptions of images and videos. 

For video content, LLaVA is incapable of direct description. Therefore, we establish a knowledge base containing bounding box coordinates for all instances within the image. Utilizing this knowledge base, the model can provide more detailed descriptions. The experimental results, accessible via the provided link, demonstrate the model's capabilities. Nevertheless, the model struggles to grasp the temporal relationships between frames. To overcome this challenge, we plan to employ a tracker to compute the trajectories of objects and incorporate this trajectory information into the knowledge base. This enhancement will equip the LMMM with the ability to understand the temporal relationships of video frames, significantly improving its descriptive accuracy and relevance.

Notebook: https://github.com/Imran2205/IST_597_003_LMMM/blob/main/agentchat_lmm_llava.ipynb